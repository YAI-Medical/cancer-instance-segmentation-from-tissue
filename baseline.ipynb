{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cancer Instance Segmentation from Tissue\n",
    "\n",
    "**YAI 2021 Fall Project - Medical Project Team**\n",
    "\n",
    "* **[Dongha Kim](https://github.com/kdha0727)**\n",
    "\n",
    "* **[Donggeon Bae](https://github.com/AttiBae)**\n",
    "\n",
    "* **[Junho Lee](https://github.com/leejunho0421)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Runtime Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mount Data Drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/kdha0727/cancer-instance-segmentation-from-tissue.git\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'cancer-instance-segmentation-from-tissue'))\n",
    "try:\n",
    "    from google.colab import drive  # NOQA\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"This notebook must be run on colab runtime!\")\n",
    "else:\n",
    "    drive.mount('/content/drive')  # NOQA\n",
    "    %cd \"/content/drive/Shareddrives/YAI 2021 가을학기 의료 프로젝트팀\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## View Runtime Information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0 or gpu_info.find('not found') >= 0:\n",
    "    import os\n",
    "    if 'TPU_NAME' in os.environ:\n",
    "        mode = 'xla'; print('TPU Runtime')\n",
    "    else:\n",
    "        mode = 'cpu'; print('Not connected to a GPU')\n",
    "else:\n",
    "    mode = 'cuda'; print(gpu_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f'Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n'\n",
    "      f'{\"Not\" if ram_gb < 20 else \"You are\"} using a high-RAM runtime!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "print(f\"OS version: \\t\\t{platform.platform()}\\nPython version:\\t\\t{sys.version.replace(chr(10), str())}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare device and library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare device\n",
    "\n",
    "if mode == 'xla':\n",
    "    try:\n",
    "        import torch_xla\n",
    "    except ImportError:\n",
    "        !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "        !python pytorch-xla-env-setup.py\n",
    "        !rm -rf pytorch-xla-env-setup.py *.whl\n",
    "        import torch_xla\n",
    "    import torch\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    # Acquires the default Cloud TPU core and moves the model to it\n",
    "    device = xm.xla_device()\n",
    "\n",
    "elif mode == 'cuda':\n",
    "    import torch\n",
    "    device = torch.device(\"cuda\")\n",
    "    # loader_kwargs = dict(pin_memory=True)\n",
    "\n",
    "else:\n",
    "    import torch\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Torch version:\\t\\t{torch.__version__}\\nTorch device:\\t\\t{device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After all installation, import all libraries used.\n",
    "\n",
    "!pip install torchinfo\n",
    "!pip install pyclean\n",
    "!pyclean .\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ChainDataset, RandomSampler, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "import torchinfo\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dataset Information**\n",
    "\n",
    "This particular directory includes training patches of size 256x256 and their masks, this is one of the folds. In total there are more than 7 thousand training patches within all the three folds.\n",
    "\n",
    "The files within each fold directories are:\n",
    "\n",
    "* `images.npy` - image patches of 256x256\n",
    "\n",
    "* `masks.npy` an array of 6 channel instance-wise masks (0: Neoplastic cells, 1: Inflammatory, 2: Connective/Soft tissue cells, 3: Dead Cells, 4: Epithelial, 6: Background)\n",
    "\n",
    "* `types.py`  tissue type that a given path was extracted from."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets.numpy_lazy import LazyNumpyDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp --verbose -r data /content/data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_length = [886, 885, 885, 841, 841, 841, 908, 907, 907]\n",
    "\n",
    "train_subset = [0, 2, 3, 4, 6, 8]\n",
    "val_subset = [1, 5]\n",
    "test_subset = [7]\n",
    "\n",
    "path_format = os.path.join(\"/content/data\", \"processed\", \"{0}\", \"{1}.npy\")\n",
    "train_dataset = ChainDataset([\n",
    "    LazyNumpyDataset(\n",
    "        image_path=path_format.format(\"images\", i),\n",
    "        mask_path=path_format.format(\"masks\", i),\n",
    "        length=batch_length[i],\n",
    "        sampler_class=RandomSampler,  # issue\n",
    "    ) for i in train_subset\n",
    "])\n",
    "val_dataset = ChainDataset([\n",
    "    LazyNumpyDataset(\n",
    "        image_path=path_format.format(\"images\", i),\n",
    "        mask_path=path_format.format(\"masks\", i),\n",
    "        length=batch_length[i],\n",
    "    ) for i in val_subset\n",
    "])\n",
    "test_dataset = ChainDataset([\n",
    "    LazyNumpyDataset(\n",
    "        image_path=path_format.format(\"images\", i),\n",
    "        mask_path=path_format.format(\"masks\", i),\n",
    "        length=batch_length[i],\n",
    "    ) for i in test_subset\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation Network\n",
    "\n",
    "**DeepLabV3 + Resnet101**: Baseline Model\n",
    "\n",
    "* **Paper**: [Arxiv 1706.05587](https://arxiv.org/abs/1706.05587)\n",
    "\n",
    "* **Implementation**: [Pytorch Vision](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)\n",
    "\n",
    "**U-Net**\n",
    "\n",
    "* **Paper**: [Arxiv 1505.04597](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "* **Implementation**: [models/unet.py](models/unet.py)\n",
    "\n",
    "**Inception U-Net**\n",
    "\n",
    "* **Paper**: [ACM 10.1145/3376922](https://dl.acm.org/doi/abs/10.1145/3376922)\n",
    "\n",
    "* **Implementation**: [models/unet.py](models/unet.py)\n",
    "\n",
    "**RefineNet**\n",
    "\n",
    "* **Paper**: [Arxiv 1611.06612](https://arxiv.org/abs/1611.06612)\n",
    "\n",
    "* **Implementation**: [models/refinenet.py](models/refinenet.py)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "from models.unet import UNet, InceptionUNet\n",
    "from models.refinenet import refinenet50, refinenet101, refinenet152, rf_lw50, rf_lw101, rf_lw152"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Baseline: DeeplabV3 + ResNet101\n",
    "\n",
    "# # Pretrained Model\n",
    "net = deeplabv3_resnet101(pretrained=True, progress=False)\n",
    "net.classifier = DeepLabHead(2048, 6)\n",
    "# net.aux_classifier = nn.Sequential()\n",
    "net.aux_classifier = FCNHead(1024, 6)\n",
    "\n",
    "# # Non-pretrained Model\n",
    "# net = deeplabv3_resnet101(pretrained=False, num_classes=6)\n",
    "\n",
    "trainable_backbone_layers = ['layer4']\n",
    "for n, p in net.named_parameters():\n",
    "    if n.startswith('backbone') and n.split('.')[1] not in trainable_backbone_layers:\n",
    "        p.requires_grad = False\n",
    "\n",
    "net.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "torchinfo.summary(net, (1, 3, 256, 256))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Network\n",
    "\n",
    "* **Binary Cross Entropy**\n",
    "\n",
    "* **Dice Coefficient**\n",
    "\n",
    "* **Intersection over Union Score**\n",
    "\n",
    "* More Multi-Label Segmentation Losses: https://jeune-research.tistory.com/entry/Loss-Functions-for-Image-Segmentation-Region-Based-Losses\n",
    "\n",
    "* See also: https://smp.readthedocs.io/en/latest/losses.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.loss import BCEDiceIoUWithLogitsLoss2d, BCEDiceIoULoss2d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Hyper Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.lr_scheduler import CosineAnnealingWarmUpRestarts\n",
    "\n",
    "# Lazy-eval iterable dataset: do not set sampler or shuffle options\n",
    "num_epoch = 100\n",
    "\n",
    "batch_size = 35\n",
    "num_workers = 1\n",
    "\n",
    "loss_function = BCEDiceIoUWithLogitsLoss2d()\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_config = {'lr': 1e-6}\n",
    "scheduler_class = CosineAnnealingWarmUpRestarts\n",
    "scheduler_config = {'T_0': 10, 'T_mult': 2, 'eta_max': 1e-3, 'T_up': 3, 'gamma': 0.5}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, num_workers=num_workers, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "optimizer = optimizer_class(net.parameters(), **optimizer_config)\n",
    "lr_scheduler = scheduler_class(optimizer, **scheduler_config)\n",
    "\n",
    "\n",
    "def load_state_dict(d):\n",
    "    net.load_state_dict(d['model'])\n",
    "    optimizer.load_state_dict(d['optimizer'])\n",
    "    lr_scheduler.load_state_dict(d['lr_scheduler'])\n",
    "\n",
    "\n",
    "def state_dict():\n",
    "    from collections import OrderedDict\n",
    "    d = OrderedDict()\n",
    "    d['model'] = net.state_dict()\n",
    "    d['optimizer'] = optimizer.state_dict()\n",
    "    d['lr_scheduler'] = lr_scheduler.state_dict()\n",
    "    return d\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import uuid\n",
    "from utils.training import train_one_epoch\n",
    "\n",
    "checkpoint_dir = f'checkpoint/{net.__class__.__name__}-{uuid.uuid4()}'\n",
    "os.makedirs('checkpoint', exist_ok=True)\n",
    "\n",
    "for ep in range(num_epoch):\n",
    "    train_one_epoch(net, loss_function, optimizer, lr_scheduler, train_loader, val_loader, device, ep, warmup_start=False)\n",
    "    torch.save(state_dict(), os.path.join(checkpoint_dir, 'epoch{}.pt').format(ep))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.evaluation import all_together, draw_confusion_matrix, show\n",
    "\n",
    "net.eval()\n",
    "\n",
    "num_workers = 4\n",
    "test_calc_loader = DataLoader(test_dataset, 64, num_workers=num_workers, drop_last=False)\n",
    "test_show_loader = DataLoader(test_dataset, 1, num_workers=num_workers, drop_last=False)\n",
    "bce, dice, iou, correct, cm = all_together(net, test_calc_loader, device=device, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"Neoplastic cells\",\n",
    "    \"Inflammatory\",\n",
    "    \"Connective/Soft tissue cells\",\n",
    "    \"Dead Cells\",\n",
    "    \"Epithelial\",\n",
    "    \"Background\"\n",
    "]\n",
    "\n",
    "draw_confusion_matrix(\n",
    "    cm[:5, :5], label_names, label_names,\n",
    "    figsize=(10, 8), title=\"Pixel-Wise Confusion Matrix\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show(net, test_show_loader, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}